{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dbd19d25",
   "metadata": {},
   "source": [
    "# Gaussian Mixture Models-Expectation Maximisation Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad1e7dcf",
   "metadata": {},
   "source": [
    "Finding a maximum likelihood solution typically requires taking the derivatives of the likelihood function with respect to all the unknown values, the parameters and the latent variables, and simultaneously solving the resulting equation.\n",
    "\n",
    "We don’t know two pieces of information simultaneously.\n",
    "\n",
    "For example, a mixture model can be described more simply by assuming that each observed data point has a corresponding unobserved data point, or latent variable, specifying the mixture component to which each data point belongs.\n",
    "\n",
    "Mixture models are probabilistic models for representing the presence of subpopulations within an overall population. \n",
    "\n",
    "Identifying which sub-population to which each individual observation belongs to.\n",
    "\n",
    "One can simply pick arbitrary values for one of two sets of unknowns, use them to estimate the second set.\n",
    "\n",
    "Use these new values to find a better estimate of the first set, and then keep alternating between the two until the resulting values both converge to fixed points. \n",
    "\n",
    "Two actual steps - Expectation and Maximization.\n",
    "\n",
    "The EM alternates between performing the E step, which creates a function of expectation of log-likelihood evaluated using the current estimate for the parameters. \n",
    "\n",
    "M- step computes parameters which maximizes the expected log-likelihood found on the E step.\n",
    "\n",
    "These parameter estimates are then used to determine the distribution of the latent variables in the next E -step. \n",
    "\n",
    "---\n",
    "\n",
    "## Gaussian Mixture Models\n",
    "\n",
    "- data from two gaussians\n",
    "- we don’t know the parameters\n",
    "- we don’t know which data point comes from which gaussian.\n",
    "- If we know at least one of them we can estimate the second.\n",
    "- EM algorithm provides solution.\n",
    "- Initialize the parameter values.\n",
    "- Find the weights.\n",
    "- Weight -given a data point what is the probability that it came from a particular distribution.\n",
    "- Weight is actually representing the coloring of data points.\n",
    "- Update the parameters.\n",
    "- Iterate the steps until convergence.\n",
    "\n",
    "## Illustration in one-dimensional case\n",
    "![image](../images/gaussian_mixture_density.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01e86abd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
