{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "62f1e335",
   "metadata": {},
   "source": [
    "## General Idea"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9699796c",
   "metadata": {},
   "source": [
    "Adaptive Boosting is an ensemble ml algorithm, that is used to classify data by combining multiple weak learners(decision trees) into a strong learner. Adaboost works by weighting the instances in the training dataset based on the accuracy of previous weak learner. \n",
    "\n",
    "Boosting algorithms work on the idea of first building a model on the trianing dataset and then bilding a second model to correct the fualts in the first model. This technique is repeated until the mistakes are reduced and the dataset is accurately predicted. \n",
    "\n",
    "There are three kinds of boosting algorithms; \n",
    "* Adaboost \n",
    "* Gradient boost\n",
    "* xgboost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "440c5d9d",
   "metadata": {},
   "source": [
    "Adaboost weak learner is a decision tree with one level, which is called as decision stumps. \n",
    "\n",
    "This approach constructs a model and assigns equal weights to all data points. It then applies larger weights to incorreclty categorised points. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1d879f6",
   "metadata": {},
   "source": [
    "## Algorithm \n",
    "* Step 1 : giving equal weights to all the instances. (1/n) \n",
    "* Step 2 : creating the first stump by selecting the lowest ginin index.\n",
    "* Step 3 : Computing the importance or influence of the stump by the following formula \n",
    "\n",
    "\\begin{align}\n",
    "\\alpha = \\frac{1}{2} \\log\\frac{1-Total Error}{Total Error}\n",
    "\\end{align}\n",
    "\n",
    "Where total error is the sum of all misclassified data points. If $\\alpha$ is 0 it represents a flawless stump, while 1 represents a bad stump. \n",
    "\n",
    "we need to determine a stump's total error and performance, so that we can update the weights since if the same weights are used in the next model, the result will be the same as it was in the previous model. \n",
    "\n",
    "The weights of incorrect forecasts will be increased, while the weights of the successful predictions will be droped.\n",
    "\n",
    "We update the weight of instances using following formula; \n",
    "\n",
    "\\begin{align}\n",
    "weight_{new} = weight_{old} * e^{\\pm\\alpha}\n",
    "\\end{align}\n",
    "\n",
    "* when a sample is successfully identified, the amount of say, will be negative, else positive. \n",
    "* To get the entire sum of weights as one, we normalize the weights by dividing all the weights by the total sum.\n",
    "\n",
    "* add buckets to each instances ->(this will results in a biased sample selection -> the misclassified points will be selected more) \n",
    "\n",
    "* we need to create a new dataset based on random values ranging from 0 to 1. \n",
    "\n",
    "* In the new dataset, the data point was incorrectly categorised has been picked three times since it has a greater weight. \n",
    "\n",
    "* we create our next stump by using the gini index."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40966ff9",
   "metadata": {},
   "source": [
    "* These steps will be continued and then we end up with mulitple decision tree stumps( sequentially). \n",
    "* when we get a test data, it goes through all the stumps. \n",
    "* final prediction will be made by aggregating all the stumps output. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec105850",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
